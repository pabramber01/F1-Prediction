{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is podium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependencies used are as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRanker\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.custom_cvs import VariableTimeSeriesSplit\n",
    "from utils.custom_scorers import balanced_accuracy_score, balanced_accuracy_ranker\n",
    "\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the tuning of the model that predicts the podium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../assets/data/processed/final_model.csv\")\n",
    "\n",
    "mid_rc = df.groupby(\"raceYear\")[\"raceRound\"].max().to_numpy() // 2\n",
    "get_half = lambda x: f'{x[\"raceYear\"]}{x[\"raceRound\"] <= mid_rc[x[\"raceYear\"] - 2006]}'\n",
    "instances_per_half = df.apply(get_half, axis=1).value_counts(sort=False).to_numpy()\n",
    "\n",
    "n_splits = len(instances_per_half) - 10\n",
    "max_train_size = [instances_per_half[i : 10 + i].sum() for i in range(n_splits)]\n",
    "test_size = instances_per_half[10:].tolist()\n",
    "tscv = VariableTimeSeriesSplit(\n",
    "    n_splits=n_splits, max_train_size=max_train_size, test_size=test_size\n",
    ")\n",
    "\n",
    "podiums = df[df[\"positionFinal\"].isin([1, 2, 3])][\n",
    "    [\"raceYear\", \"raceRound\", \"driverRef\"]\n",
    "]\n",
    "podiums = podiums.groupby(by=[\"raceYear\", \"raceRound\"]).agg({\"driverRef\": \",\".join})\n",
    "\n",
    "X = pd.read_csv(\"../assets/data/processed/final_model_X.csv\")\n",
    "y = df.merge(podiums, how=\"left\", on=[\"raceYear\", \"raceRound\"], suffixes=(\"\", \"Podium\"))\n",
    "y = y.apply(lambda x: int(x[\"driverRef\"] in x[\"driverRefPodium\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 0.7712286583402587 with {'metric': 'cosine', 'n_neighbors': 9,\n",
      "\t'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "grid = dict(\n",
    "    n_neighbors=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 21, 31, 51, 101],\n",
    "    weights=[\"uniform\", \"distance\", None],\n",
    "    metric=[\"euclidean\", \"manhattan\", \"cosine\"],\n",
    ")\n",
    "search = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    grid,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    ").fit(X, y)\n",
    "output = f\"KNeighborsClassifier: {search.best_score_} with {search.best_params_}\"\n",
    "print(\"\\n\".join(textwrap.wrap(output, 88, subsequent_indent=\"\\t\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier: 0.806103572882649 with {'criterion': 'entropy', 'max_depth': 3,\n",
      "\t'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "grid = dict(\n",
    "    criterion=[\"gini\", \"entropy\", \"log_loss\"],\n",
    "    splitter=[\"best\", \"random\"],\n",
    "    max_depth=[2, 3, 4, 5, 6, 10, 20],\n",
    ")\n",
    "search = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    grid,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    ").fit(X, y)\n",
    "output = f\"DecisionTreeClassifier: {search.best_score_} with {search.best_params_}\"\n",
    "print(\"\\n\".join(textwrap.wrap(output, 88, subsequent_indent=\"\\t\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier: 0.803026762510689 with {'criterion': 'gini', 'max_depth': 6,\n",
      "\t'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grid = dict(\n",
    "    n_estimators=[10, 30, 50, 100, 200],\n",
    "    criterion=[\"gini\", \"entropy\", \"log_loss\"],\n",
    "    max_depth=[2, 3, 4, 5, 6, 10, 20],\n",
    ")\n",
    "search = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    grid,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    ").fit(X, y)\n",
    "output = f\"RandomForestClassifier: {search.best_score_} with {search.best_params_}\"\n",
    "print(\"\\n\".join(textwrap.wrap(output, 88, subsequent_indent=\"\\t\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier: 0.8079596679237415 with {'activation': 'logistic', 'hidden_layer_sizes':\n",
      "\t(50, 20, 5)}\n"
     ]
    }
   ],
   "source": [
    "grid = dict(\n",
    "    hidden_layer_sizes=[(100,), (50, 25), (50, 20, 5)],\n",
    "    activation=[\"relu\", \"logistic\"],\n",
    ")\n",
    "search = GridSearchCV(\n",
    "    MLPClassifier(),\n",
    "    grid,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    ").fit(X, y)\n",
    "output = f\"MLPClassifier: {search.best_score_} with {search.best_params_}\"\n",
    "print(\"\\n\".join(textwrap.wrap(output, 88, subsequent_indent=\"\\t\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: 0.8126726830996678 with {'subsample': 0.75, 'reg_lambda': 10,\n",
      "\t'reg_alpha': 10, 'n_estimators': 50, 'min_child_weight': 15, 'max_depth': 3,\n",
      "\t'learning_rate': 0.2, 'gamma': 0.5, 'colsample_bytree': 0.9}\n"
     ]
    }
   ],
   "source": [
    "grid = dict(\n",
    "    learning_rate=[0.01, 0.1, 0.2],\n",
    "    n_estimators=[50, 75, 150],\n",
    "    max_depth=[3, 5, 10],\n",
    "    min_child_weight=[1, 5, 15, 200],\n",
    "    gamma=[0, 0.5, 0.75, 0.9],\n",
    "    subsample=[0.5, 0.75, 0.9],\n",
    "    colsample_bytree=[0.5, 0.75, 0.9],\n",
    "    reg_alpha=[0, 3, 10],\n",
    "    reg_lambda=[0, 3, 10],\n",
    ")\n",
    "search = RandomizedSearchCV(\n",
    "    XGBClassifier(objective=\"binary:logistic\"),\n",
    "    grid,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    "    n_iter=30,\n",
    ").fit(X, y)\n",
    "output = f\"XGBClassifier: {search.best_score_} with {search.best_params_}\"\n",
    "print(\"\\n\".join(textwrap.wrap(output, 88, subsequent_indent=\"\\t\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor: 0.825580718403798 with {'subsample': 0.75, 'reg_lambda': 0, 'reg_alpha':\n",
      "\t0, 'n_estimators': 50, 'min_child_weight': 200, 'max_depth': 3, 'learning_rate': 0.1,\n",
      "\t'gamma': 0, 'colsample_bytree': 0.75}\n"
     ]
    }
   ],
   "source": [
    "grid = dict(\n",
    "    learning_rate=[0.01, 0.1, 0.2],\n",
    "    n_estimators=[50, 75, 150],\n",
    "    max_depth=[3, 5, 10],\n",
    "    min_child_weight=[1, 5, 15, 200],\n",
    "    gamma=[0, 0.5, 0.75, 0.9],\n",
    "    subsample=[0.5, 0.75, 0.9],\n",
    "    colsample_bytree=[0.5, 0.75, 0.9],\n",
    "    reg_alpha=[0, 3, 10],\n",
    "    reg_lambda=[0, 3, 10],\n",
    ")\n",
    "search = RandomizedSearchCV(\n",
    "    XGBRegressor(objective=\"reg:linear\"),\n",
    "    grid,\n",
    "    scoring=make_scorer(balanced_accuracy_score),\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    "    n_iter=50,\n",
    ").fit(X, y)\n",
    "output = f\"XGBRegressor: {search.best_score_} with {search.best_params_}\"\n",
    "print(\"\\n\".join(textwrap.wrap(output, 88, subsequent_indent=\"\\t\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRanker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRanker: 0.8438580134166631 with {'subsample': 0.5, 'reg_lambda': 3, 'reg_alpha': 0,\n",
      "\t'n_estimators': 150, 'min_child_weight': 15, 'max_depth': 5, 'learning_rate': 0.01,\n",
      "\t'gamma': 0.9, 'colsample_bytree': 0.5}\n"
     ]
    }
   ],
   "source": [
    "races_per_year = np.cumsum([0] + df.groupby(\"raceYear\")[\"raceRound\"].max().to_list())\n",
    "set_id = lambda y, r: r + (races_per_year[y - 2006])\n",
    "\n",
    "X[\"qid\"] = df.apply(lambda x: set_id(x[\"raceYear\"], x[\"raceRound\"]), axis=1)\n",
    "\n",
    "grid = dict(\n",
    "    learning_rate=[0.01, 0.1, 0.2],\n",
    "    n_estimators=[50, 75, 150],\n",
    "    max_depth=[3, 5, 10],\n",
    "    min_child_weight=[1, 5, 15, 200],\n",
    "    gamma=[0, 0.5, 0.75, 0.9],\n",
    "    subsample=[0.5, 0.75, 0.9],\n",
    "    colsample_bytree=[0.5, 0.75, 0.9],\n",
    "    reg_alpha=[0, 3, 10],\n",
    "    reg_lambda=[0, 3, 10],\n",
    ")\n",
    "search = RandomizedSearchCV(\n",
    "    XGBRanker(objective=\"rank:pairwise\"),\n",
    "    grid,\n",
    "    scoring=balanced_accuracy_ranker,\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    "    n_iter=50,\n",
    ").fit(X, y)\n",
    "output = f\"XGBRanker: {search.best_score_} with {search.best_params_}\"\n",
    "print(\"\\n\".join(textwrap.wrap(output, 88, subsequent_indent=\"\\t\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After viewing several runs, the hyperparameters for each algorithm are as follows\n",
    "\n",
    "- KNeighborsClassifier: 0.7712286583402587 with {'metric': 'cosine', 'n_neighbors': 9, 'weights': 'distance'}\n",
    "- DecisionTreeClassifier: 0.806103572882649 with {'criterion': 'entropy', 'max_depth': 3, 'splitter': 'best'}\n",
    "- RandomForestClassifier: 0.803026762510689 with {'criterion': 'gini', 'max_depth': 6, 'n_estimators': 100}\n",
    "- MLPClassifier: 0.8079596679237415 with {'activation': 'logistic', 'hidden_layer_sizes': (50, 20, 5)}\n",
    "- XGBClassifier: 0.8126726830996678 with {'subsample': 0.75, 'reg_lambda': 10, 'reg_alpha': 10, 'n_estimators': 50, 'min_child_weight': 15, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 0.5, 'colsample_bytree': 0.9}\n",
    "- XGBRegressor: 0.825580718403798 with {'subsample': 0.75, 'reg_lambda': 0, 'reg_alpha': 0, 'n_estimators': 50, 'min_child_weight': 200, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.75}\n",
    "- XGBRanker: 0.8438580134166631 with {'subsample': 0.5, 'reg_lambda': 3, 'reg_alpha': 0, 'n_estimators': 150, 'min_child_weight': 15, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.9, 'colsample_bytree': 0.5}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
